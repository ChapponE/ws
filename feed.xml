<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://chappone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chappone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-06T01:03:44+00:00</updated><id>https://chappone.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Colour transfert with optimal transport method</title><link href="https://chappone.github.io/blog/2023/color_transport/" rel="alternate" type="text/html" title="Colour transfert with optimal transport method"/><published>2023-11-05T15:12:00+00:00</published><updated>2023-11-05T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2023/color_transport</id><content type="html" xml:base="https://chappone.github.io/blog/2023/color_transport/"><![CDATA[<h2><u>Introduction</u></h2> <p>The objective is to transfer the colormap of an image to make it match that of a target image using optimal transport theory. The code to reproduce the results is available on my <a href="https://github.com/ChapponE/optimal_transport_color_transfer">GitHub account</a>. The algorithm developed and theory come from the articles <a href="https://mural.maynoothuniversity.ie/15125/">Automated colour grading using colour distribution transfer</a> and <a href="https://www.semanticscholar.org/paper/Wasserstein-Barycenter-and-Its-Application-to-Rabin-Peyr%C3%A9/9b208891d1287ebb5b84ac801b41c3313d7e3303">Wasserstein Barycenter and Its Application to Texture Mixing</a>.</p> <h2><u>Representation of the images</u></h2> <p>Because we are interested only in the colors, one image is modeled as a point cloud of \(\mathbb{R}^3\) with a number of points equal to the number of pixels in the image. This means that \(X=\{x_i\}_{i \in [1,N]}\) with \(x_i \in \mathbb{R}^3\) represents an image. If we permute the pixels of an image, it’s representation remains the same. Thus we work in: <br/> <span style="display: block; text-align: center;">\(\mathcal{M}=\{[X]; X \in (\mathbb{R}^3)^N \}\) with \([X]=\{(X_{\sigma(j)})_{j=[1,N]}, \sigma \in \Sigma_N\}\), \(\Sigma_N\) is the set of all permutations.</span></p> <h2><u>Wasserstein Distance</u></h2> <p>We define the Wasserstein distance on \(\mathcal{M}\) :<br/> <span style="display: block; text-align: center;">\(\forall X,Y \in \mathcal{M}, W(X, Y)^{2}={\min\limits_{\sigma \in \Sigma_{N}}} W_{\sigma}(X, Y) \quad \text { where } \quad W_{\sigma}(X, Y)={\sum\limits_{i \in I}}\left\|X_{i}-Y_{\sigma(i)}\right\|^{2}\) </span> This minimization problem can be computationally expensive, but we can leverage the scalar case (when \(X_i\) and \(Y_i\) are in \(\mathbb{R}\) for \(i \in [1, N]\) instead of being in \((\mathbb{R}^3)^N\)).<br/> In this case \(W(X, Y)=W_{\sigma^*}(X, Y)\) with \(\sigma^*=\sigma_Y \circ \sigma_X^{-1}\) where :<br/> <span style="display: block; text-align: center;">\(\forall 1 \leq i \leq N, \quad X_{\sigma_{X}(i)} \leq X_{\sigma_{X}(i+1)} \quad \text { and } \quad Y_{\sigma_{Y}(i)} \leqslant Y_{\sigma_{Y}(i+1)}\)</span> It is not computationally costly to calculate \(\sigma\) with fast sorting algorithms. <br/> Then we define the sliced Wasserstein Distance \(\tilde{W}\) which is calculated with only 1-D Wasserstein Distances and approximates the wesserstein Distance as follows:<br/> <span style="display: block; text-align: center;">\(\forall X, Y \in \mathcal{M}, \tilde{W}(X, Y)^{2}=\int_{\theta \in S^2} {\min\limits_{\sigma_{\theta} \in \Sigma_{N}}} {\sum\limits_{i \in I}} \left\langle X_{i}-Y_{\sigma_{\theta}(i)}, \theta\right\rangle^{2} \mathrm{~d} \theta\)</span> This calculate the Wasserstein Distance using the projected vectors in a certain direction of \(\mathbb{R}^3\) and integrates over all directions. For more details, you can refer to the articles mentioned in the introduction.</p> <h2><u>Barycenter in Wasserstain Space</u></h2> <p>Here we define the barycenter in this space and provide an algorithm to obtain the barycenter of the image we want to transport, weighted at 0, and the image with the desired colormap, weighted at 1. <br/> By analogy with the barycenter in Euclidean space, the barycenter in \((\mathcal{M}, \tilde{W})\) is defined as :<br/> \(\operatorname{Bar}\left(\lambda_{j}, Y^{j}\right)_{j \in J} \in \underset{X}{\operatorname{argmin}} E(X)\) where \(E=\sum_{j \in J} \lambda_{j} \tilde{W}\left(X_\theta, Y_\theta^{j}\right)^{2}=\int_{\theta \in S^2}{\sum\limits_{j \in J}}\lambda_j \left\langle X_{i}-Y_{\sigma^*_{\theta}(i)}, \theta\right\rangle^{2} \mathrm{~d} \theta\)<br/> With \(X_\theta=\{&lt;X_i,\theta&gt;\}_{i\in[1,N]}\subset \mathbb{R}\) and the \(\sigma^*_\theta\) defined in Wasserstein Distance part.</p> <h2><u>The transportation algorithm:</u></h2> <p>We apply the Wasserstein barycenter with \(\{(0,X);(1,Y)\}\) with \(X\) as the original image (Image 1 in the following figure) we want to transform and \(Y\) as the target image (one of the 5 others) that we want to use to transform \(X\) and use colors.</p> <div style="text-align: center"><img src="/assets/img/ot_6img.PNG" alt="A sample image" width="600"/></div> <p>\(\,\) <br/> Thus the transported image is defined as:<br/> \(\begin{align} X^*=\arg{\min\limits_X}\int_{\theta \in S^2}W(X_\theta, Y_\theta)^2 \mathrm{~d} \theta\end{align}\)<br/> We can use gradient descent to find an approximation of this minimizer:<br/> <span><span style="font-weight: bold;">Gradient descent to minimize (1):</span><br/> <span style="font-weight: bold;">Assumption:</span> Images \(X\) and \(Y\) have the same number of pixels.<br/> <span style="font-weight: bold;">Input:</span> \(X\), \(Y\) and a stepsize \(\epsilon\)<br/> \(X^{0}=X\)<br/> <span style="font-weight: bold;">For</span> \(k = 0, 1, 2, \ldots\):<br/> \(\quad \quad X^{k}=X^{k}-\epsilon\nabla W(X^{k}_\theta, Y_\theta)\) with \(\theta\) a realization of an uniform random variable in \(S^2\)<br/> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{k\rightarrow\infty}{X^k} \in \arg{\min\limits_X}\int_{\theta \in S^2}W(X_\theta, Y_\theta)^2 \mathrm{~d} \theta\) </span></p> <ul> <li>Remark: At each step, we pick a direction in the color space and take a step of gradient descent to make the colormap of the current iteration closer to the one of \(Y\) in that direction.</li> </ul> <h2><u>Results</u></h2> <p>Here are the results obtained by applying the algorithm with 100 iterations and \(\epsilon=1\) to transport the first image, \(X\), to the colormap of the 5 other images, denoted as \(Y\):</p> <p><img src="/assets/img/Transported_images.png" alt="A sample image" width="1300" style="margin-left: -300px;"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[This project aims to match the color palette of an original image with that of a target image. It was developed as part of a practical assignment in an optimal transport class.]]></summary></entry><entry><title type="html">Unfolded neural network for deblur and denoise</title><link href="https://chappone.github.io/blog/2023/mat/" rel="alternate" type="text/html" title="Unfolded neural network for deblur and denoise"/><published>2023-10-16T15:12:00+00:00</published><updated>2023-10-16T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2023/mat</id><content type="html" xml:base="https://chappone.github.io/blog/2023/mat/"><![CDATA[<h2><u>Introduction</u></h2> <p>This project aims to deblur and denoise images, with a focus on the MNIST dataset. The methodology is based on the “Unfolded Forward Backward” approach, developed as part of <a href="https://chappone.github.io/ws/assets/pdf/circumstellar.pdf">my master’s thesis project</a> at ENS Lyon. The code to produce the results of this blog post is available on my <a href="https://github.com/ChapponE/img_deblur">github account</a>.</p> <h2><u>Data</u></h2> <p>I worked with the standard MNIST dataset imported from the Keras library, consisting of 70,000 images, each of size \(28 \times 28\) pixels. To normalize these images, I divided each pixel’s value by 255 to bring them into the range of 0 to 1. To degrade the quality of the handwritten digits, I applied a Gaussian blur with a kernel size of \(7 \times 7\) and a standard deviation of 0.5. Subsequently, I introduced Gaussian noise with a standard deviation of 0.25, resulting in pairs of original and degraded images, as displayed below. We measured the difference between the degraded and original images using the traditional PSNR metric.</p> <div style="text-align: center"><img src="/assets/img/sample.png" alt="A sample image" width="320"/></div> <p>The main objective of this project is to create an estimator for the original image based on the degraded image. The quality of this estimator is assessed using the PSNR metric.</p> <h2><u>Model</u></h2> <p>\(\bullet\) I adopted the classical formalism of inverse problems:<br/> The goal of solving an \(\textit{inverse problem}\) is to create an estimator, denoted as \(\boldsymbol{\widehat{x}} \in \mathbb{R}^{28^2}\), from a set of observed measurements, represented as the degraded images \(\boldsymbol{y} \in \mathbb{R}^{28^2}\). The objective is to create an estimator that is close to the ground truth data, the original image \(\boldsymbol{\bar{x}}\). In our case, we have \(\boldsymbol{y} = A\boldsymbol{\bar{x}} + \mathbf{\epsilon}, \text{ with } A \in \mathbb{R}^{28^2 \times 28^2} \text{ and } \mathbf{\epsilon} \sim \mathbb{N}(\mathbf{0}, \mathbf{\sigma^2 I})\). With \(A\) the gaussian blur and \(\mathbf{\epsilon}\) the gaussian noise.</p> <p>\(\bullet\) The estimator \(\boldsymbol{\widehat{x}}\) for \(\boldsymbol{\bar{x}}\) is typically obtained by solving a variational problem, defined as follows: \(\begin{align} \widehat{\boldsymbol{x}} \in \underset{\boldsymbol{x} \in \mathbb{R}^{28^2}}{\arg \min}\left[\frac{1}{2}||\boldsymbol{y}-A\boldsymbol{x}||_2^2+\mathcal{P}(\boldsymbol{x})\right] \end{align}\) Here, \(||\boldsymbol{y}-A\boldsymbol{x}||_2^2\) represents the data fidelity term, ensuring that the degraded solution \(A\boldsymbol{\widehat{x}}\) is close to the degraded signal \(\boldsymbol{y}\). Additionally, \(\mathcal{P}: \mathbb{R}^{28^2} \rightarrow \mathbb{R}\) is a prior term. For more details on variational problems, you can refer to my Master’s thesis.</p> <p>\(\bullet\) Choosing an appropriate prior term can be challenging. However, once a suitable prior term is chosen, it is possible to find the minimizer of equation (1) using proximal iterative methods like Forward-Backward:<br/> <span><span style="font-weight: bold;">Forward-Backward algorithm:</span><br/> <span style="font-weight: bold;">Assumption:</span> \(\zeta := |||A^*A|||_2&lt;2,\)<br/> <span style="font-weight: bold;">Input:</span> \(\boldsymbol{y}\in \mathbb{R}^{28^2}\), step size \(\tau\in ]0,2\zeta^{-1}[,\)<br/> \(x^0 = A^*y,\)<br/> <span style="font-weight: bold;">For</span> \(n = 0, 1, 2, \ldots\):<br/> \(\quad \quad \tilde{x}^{n}=x^n-\tau A^*(Ax^n - \boldsymbol{y})\);<br/> \(\quad \quad x^{n+1} = \operatorname{prox}_{\tau \mathcal{R}}(\tilde{x}^{n})\);<br/> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{n\rightarrow\infty}{x^n} \in{\arg\min\limits_{x \in \mathbb{R}^{28^2}}}\left[\dfrac{1}{2}||\boldsymbol{y}-A\boldsymbol{x}||_2^2+\mathcal{P}(\boldsymbol{x})\right]\) </span></p> <ul> <li>Remark: Let \(h: \mathcal{H} \rightarrow \mathbb{R},\) \(\forall x \in \mathcal{H} \quad \operatorname{prox}_{\tau h}(x)=\underset{y \in \mathcal{H}}{\operatorname{argmin}} \frac{1}{2 \tau}\|x-y\|^{2}+h(y)\)</li> </ul> <p>\(\bullet\) To address the challenge of selecting a suitable prior term, the iterations of Forward-Backward can be unfolded to learn the prior through deep-learning frameworks. The first unfolded network was introduced in the paper <a href="https://icml.cc/Conferences/2010/papers/449.pdf"> Learning Fast Approximations of Sparse Coding </a> in 2010. To parameterize the unfolded network, I replaced the proximal operator of the prior with UNets, denoted as \(U_{\theta^k}\). To facilitate the learning process, I limited the number of iterations to 5. The algorithm is defined as follows:\(\\\)</p> <p><span><span style="font-weight: bold;">Unfolded Forward-Backward architecture network:</span><br/> <span style="font-weight: bold;">Assumption:</span> \(\zeta:=|||A^*A|||_2 &lt; 2,\, {U_{\theta^k}}_{k\in[0;4]},\) UNets,<br/> <span style="font-weight: bold;">Input:</span> \(\boldsymbol{y}\in \mathbb{R}^{28^2}\), step size \(\tau \in ]0,2\zeta^{-1}[,\)<br/> \(x^0 = A^*y,\)<br/> <span style="font-weight: bold;">For</span> \(n = 0, 1, 2, \ldots 4\):<br/> \(\quad \quad \tilde{x}^{n}=x^n- \tau A^*(y - Ax^n);\)<br/> \(\quad \quad x^{n+1} = U_{\theta^{n+1}}(\tilde{x}^{n})\);<br/> <span style="font-weight: bold;">Output:</span> \(f_\theta(y)=x^5\) </span></p> <p>\(\bullet\) I trained the unfolded network using \(300\) of the \(70\,000\) images from the MNIST dataset. I reserved \(10\,000\) images for testing. It is possible to play with the number of iterations, which corresponds to the layers of our neural network, and the number of scales of the UNets, which determine the size of the UNets. I kept the number of layers fixed at 5 and the number of scales within the set \(\{2, 3, 4\}\).</p> <h2><u>Results</u></h2> <p>\(\bullet\) I minimized the cost function, which is defined as the mean squared error on the \(300\) training images: \(\sum\limits_{i \in [1,300]} \|\bar{x}_i - f_\theta(y)\|^2_2\).</p> <p>\(\bullet\) In the following plot, the training was performed for 50 epochs using the Adam algorithm. The red curve represents the PSNR calculated on the test set, and the blue curve represents the value of the loss function with respect to the number of epochs. Additionally, a table displaying the mean PSNR on the test set, the number of parameters, and the training time is provided.</p> <div style="text-align: center"><img src="/assets/img/curves_mnist.PNG" alt="A sample image" width="820"/></div> <div style="text-align: center"><img src="/assets/img/table_MNIST.png" alt="A sample image" width="490"/></div> <p>\(\bullet\) It is evident that there is no significant difference between the three settings. This can be attributed to the relatively low number of training epochs. In my thesis, it was observed that a higher number of parameters leads to better PSNR results. However, training for hundreds of epochs on my personal computer, which lacks a GPU, is time-consuming. Additionally, it’s noteworthy that the training time increases with a higher number of scales. <br/> Here is an example of a reconstructed sample, demonstrating the quality of the results:</p> <div style="text-align: center"><img src="/assets/img/sample_mnist.png" alt="A sample image" width="820"/></div>]]></content><author><name></name></author><summary type="html"><![CDATA[This project aims to deblur and denoise images, with a focus on the MNIST dataset. The methodology is based on the "Unfolded Forward Backward" approach, developed as part of my master's thesis project at ENS Lyon.]]></summary></entry></feed>