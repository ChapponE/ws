<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://chappone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chappone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-04T15:20:18+00:00</updated><id>https://chappone.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Discord bot</title><link href="https://chappone.github.io/blog/2024/bot/" rel="alternate" type="text/html" title="Discord bot"/><published>2024-01-29T15:12:00+00:00</published><updated>2024-01-29T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2024/bot</id><content type="html" xml:base="https://chappone.github.io/blog/2024/bot/"><![CDATA[<h2><u>Introduction</u></h2> <p>This post introduces my first Discord bot designed to remind users of upcoming events throughout the week while automatically updating the weekly message. The Python code for this bot is accessible on my <a href="https://github.com/ChapponE/event_bot/tree/master">github</a>.</p> <h2><u>How to use</u></h2> <p>After pulling the GitHub project, you need to create a ‘.env’ file at the root of your bot folder like on the screenshot right below. This file must include the following three variables: DISCORD_API_TOKEN, channel_id, and guild_id. You can find the DISCORD_API_TOKEN in your application bot settings, while the channel_id and guild_id can be obtained from your Discord guild and channel settings, respectively. Ensure to select the channel_id of the desired channel where you want the bot to send messages every week.</p> <p><img src="/assets/img/env_file.PNG" alt="A sample image" style="width: 100%"/></p> <p>Then, you can run main.py to use the bot.</p> <h2><u>Customize</u></h2> <p>It is possible to customize the bot by modifying the variables below in the main.py file:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh"># Constants</span>
DAY = 6  # The day on which the weekly reminder is triggered (0 is Monday, 6 is Sunday)
HOUR = 12  # The hour (UTC) at which the weekly reminder is triggered
MIN_MIN = 20  # The minimum minute within the HOUR for the weekly reminder
MIN_MAX = 40  # The maximum minute within the HOUR for the weekly reminder
DAYS_INTERVAL = 7  # The number of days into the future for considering events
NB_MINUTES_LIST_EVENTS_SLEEP = MIN_MAX - MIN_MIN + 1  # Interval to sleep after posting events in the channel
NB_MINUTES_WEEKLY_REMINDER_LOOP = MIN_MAX - MIN_MIN - 1  # Interval for the weekly reminder loop
NB_MINUTES_CHECK_CHANGES_LOOP = 20  # Interval to check for changes in events

<span class="gh"># Messages</span>
intro_txt = "Bonjour à toustes ! Voici les événements de cette fin de semaine au ZincADit, n'hésitez pas à venir bénévoler ❤️ :<span class="se">\n</span> <span class="se">\n</span>"
no_event = "Il n'y a pas encore d'événement programmé dans les prochains jours. <span class="se">\n\n</span> Retrouvez toutes les informations des évenements sur : https://www.zincadit.beer/"
conclusion_txt = "Retrouvez toutes les informations des évenements sur : https://www.zincadit.beer/"
</code></pre></div></div> <p>The basic settings are configured to send a message every Sunday at noon, between 20 and 40 minutes past the hour. This message includes events scheduled for the upcoming 7 days. A sleeping time of 21 minutes (40-20+1) is set to ensure that the message is not sent twice. The “weekly_reminder” loop is checked every 19 minutes (40-20-1) to determine if it is time to send a new message. Changes are monitored, and the message is updated every 20 minutes.</p> <p>Additionally, you need to customize the message before events using “intro_txt,” the message after events with “conclusion_txt,” and the message for when there are no events with “no_event.”</p> <p>When there are events it will send a message like this:</p> <p><img src="/assets/img/discord_message.PNG" alt="A sample image" style="width: 100%"/></p> <p>When there are no event it will send a message like this:</p> <p><img src="/assets/img/message_no_event.PNG" alt="A sample image" style="width: 100%"/></p> <p>A second set of constants is commented. Those variables are setted to test the bot. You just need to set “DAY_” and “HOUR_” accordingly to the current day (0=monday, …) and current hour and can test that messages are well edited few seconds after modifying events title, date or description.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post presents my first Discord bot designed to remind users of upcoming events throughout the week while automatically updating the weekly message.]]></summary></entry><entry><title type="html">Origins of unfolded networks and theory; Learning Iterative Soft Thresholding Algorithm (LISTA)</title><link href="https://chappone.github.io/blog/2023/lista/" rel="alternate" type="text/html" title="Origins of unfolded networks and theory; Learning Iterative Soft Thresholding Algorithm (LISTA)"/><published>2023-12-03T15:12:00+00:00</published><updated>2023-12-03T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2023/lista</id><content type="html" xml:base="https://chappone.github.io/blog/2023/lista/"><![CDATA[<h2><u>Introduction</u></h2> <p>The objective is to implement the first unfolded Neural Network (NN) described in <a href="https://icml.cc/Conferences/2010/papers/449.pdf">Learning Fast Approximations of Sparse Nonlinear Regression</a>. I also implement 3 variants of it introduced in <a href="https://arxiv.org/abs/1808.10038"> Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds </a> which gives the proof of 3 theorems we will explain and empiricaly verify. <br/> The code used for this project is available on <a href="https://github.com/ChapponE/LISTA">GitHub</a>. I used other unroled network to learn how to deblur and denoise images from mnist daset in an <a href="https://chappone.github.io/ws/blog/2023/mat/">other post blog<a>.</a></a></p> <h2><u>Sparse signal recovery and Data</u></h2> <p>In this blog post we will solve an sparse signal recovery problem. We aim to recover a sparse vector \(x^* \in \mathbb{R}^n\) from its noisy linear measurments \(y \in \mathbb{R}^m\), with \(m=250\) and \(n=500\): <span style="display: block; text-align: center;"> \(y=Ax^*+\epsilon\)</span> Where \(\epsilon \in \mathbb{R}^m\) an additive Gaussian white noise to have a Signal to Noise Rate (SNR) of \(30dB\), and \(A \in \mathbb{R}^{m\times n}\) with each entries sampled as Gaussian distribution \(A_{i,j}∼\mathcal{N}(0,1/m)\) and then we normalize the columns.<br/> We define \(x^*\) as a Gaussian vector of \(\mathbb{R}^n\) where we set each entry to \(0\) with a probability of \(p_b=0.1\) to make it sparse. \(1\,000\) such data are generated for the train set and \(1\,000\) others for the test set.</p> <h2><u>Iterative Shrinkage Thresholding Algorithm (ISTA)</u></h2> <p>In inverse problem, the estimator \(\widehat{x}\) of \(x^*\) is classically obtained by the minimization of the sum of a datafidelity term \(||y-Ax||_2^2\) and the regularization \(\lambda \|x\|_1\) to enforce the sparsity of the solution. This problem is known as LASSO problem:<br/> \(\begin{align} \widehat{x} \in \underset{x \in \mathbb{R}^{n}}{\arg\min}\left[\dfrac{1}{2}||y-Ax||_2^2+ \lambda \|x\|_1 \right] \end{align}\)</p> <p>ISTA is an iterative algorithm which aims to solve \((1)\), it converges sublinearly (see <a href="https://nikopj.github.io/blog/understanding-ista/">the link</a> for more explanations of that algorithm):</p> <p><span><span style="font-weight: bold;">ISTA:</span><br/> <span style="font-weight: bold;">Assumption:</span> \(\zeta := |||A^*A|||_2&lt;2,\)<br/> <span style="font-weight: bold;">Input:</span> \(y\in \mathbb{R}^{m}\), step size \(\tau\in ]0,2\zeta^{-1}[,\)<br/> \(x^0 = A^*y,\)<br/> <span style="font-weight: bold;">For</span> \(k = 0, 1, 2, \ldots\):<br/> \(\quad \quad \tilde{x}^{k}=x^n-\tau A^*(Ax^k - y)\)<br/> \(\quad \quad x^{k+1} = \mathcal{S}_{\lambda/L}(\tilde{x}^{k}) \quad \quad \text{with} \; \;(\mathcal{S}_{\theta}(x))_i = \operatorname{sgn}(x_i)\operatorname{max}(|x_i|-\theta,0)\)<br/> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{k\rightarrow\infty}{x^k} \in{\arg\min\limits_{x \in \mathbb{R}^{n}}}\left[\dfrac{1}{2}||y-Ax||_2^2+\lambda||x||_1\right]\) </span></p> <ul> <li>Remark: \(\mathcal{S}_\theta\) is called the soft thresholding function. It is a non-linear function.</li> </ul> <h2><u>Original LISTA</u></h2> <p>A neural network \(d_\theta\) with \(K\) layers and parametrized by \(\theta\) is defined as:</p> \[\begin{align} d_\theta(y) = \eta^{K}\left(W^{K}_2 \dots \eta^{1}\left(W^{1}_2y + W^{1}_1\right) \dots + W^{K}_1\right) \quad\quad \text{with} \quad \theta = \{W^{1}_2, \ldots, W^{K}_2, W^{1}_1, \ldots, W^{K}_1\} \end{align}\] <p>The article <a href="https://arxiv.org/abs/2010.13490">Learning Fast Approximations of Sparse Nonlinear Regression</a> introduce the first unrolled neural network called Learned ISTA (LISTA) by hilighting the fact that an iteration of ISTA looks like a layer of a neural network if we rearange iterations of ISTA as below and we assimilate \(\mathcal{S}_{\theta^k}\) as the activation function:</p> <div style="text-align: center"> <img src="/assets/img/ista_vs_nn.PNG" alt="A sample image" style="width: 550px"/> </div> <p>We fix number of layers \(K=16\) and \(x^0=\vec{0}\) LISTA network is defined as :</p> \[\begin{align} d_\theta^K(y) = S_{\theta^K}\left(W^{K}_2 \dots S_{\theta^1}\left(W^{1}_2x^0 + W^{1}_1y\right) \dots + W^{K}_1y\right) \quad \quad \text{with} \quad \theta = \{W^{1}_2, \ldots, W^{K}_2, \theta^1, \ldots, \theta^K, W^{1}_1, \ldots, W^{K}_1\} \end{align}\] <ul> <li>Remarks: Advantage of LISTA compare to ISTA is that when the parameters \(\theta\) is trained it is cheap \(\quad \quad\quad \;\) to evaluate. ISTA requires thousands of iterations where LISTA requires only few tens. \(\quad \quad\quad \;\) This network and ones we will see further have been trained with Adam for \(2\,500\) epochs. \(\quad \quad\quad \;\) With thoses settings, LISTA has \(6\,000\,016\) parameters to train.</li> </ul> <p>To start with a good initialization and make training easier, we start training with :</p> <div style="text-align: center"> $$\begin{align*} \forall k \in [1,\ldots, K], \quad&amp;\theta^k = \lambda/L \\ &amp; W_2^k = (\mathbb{I}-\tau A^*A) \quad \quad \text{with} \quad \tau=L^{-1}\\ &amp; W_1^k = -\tau A^* \end{align*}$$ </div> <p>Here are the curves of loss and PSNR of the traini and validation/test sets (which contains \(1\,000\) data each) during the training phase. The next parts explains and illustrates theoriticals results of convergence for LISTA networks and 3 variations.</p> <div style="text-align: center"> <img src="/assets/img/lista_train_curves.PNG" alt="A sample image" style="width: 550px"/> </div> <h2><u>Necessary condition for LISTA convergence</u></h2> <p>Theorems are prooved in the paper <a href="https://arxiv.org/abs/1808.10038"> Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds</a>.</p> <p>\(\textbf{Assumption 1:}\) The signal \(x^{*}\) and the observation noise \(\epsilon\) are sampled from the following set:</p> \[\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma) \triangleq\left\{\left(x^{*}, \varepsilon\right)|| x_{i}^{*} \mid \leq B, \forall i,\left\|x^{*}\right\|_{0} \leq s,\|\varepsilon\|_{1} \leq \sigma\right\} .\] <p>\(\textbf{Theorem 1:}\) Given \(\theta=\left\{W_{1}^{k}, W_{2}^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(y\) be observed by (1) and \(\left\{d^k_\theta\right\}_{k=1}^{\infty}\) be generated layer-wise by LISTA (3). If the following holds uniformly for any \(\left(x^{*}, \epsilon\right) \in \mathcal{X}(B, s, 0)\):</p> \[d^k_\theta(y) \rightarrow x^{*}, \quad \text { as } k \rightarrow \infty\] <p>and \(\left\{W_{2}^{k}\right\}_{k=1}^{\infty}\) are bounded:</p> \[\left\|W_{2}^{k}\right\|_{2} \leq B_{W}, \quad \forall k=0,1,2, \cdots\] <p>then \(\theta=\left\{W_{1}^{k}, W_{2}^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) must satisfy:</p> \[\begin{aligned} &amp; W_{2}^{k}-\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow 0, \quad \text { as } k \rightarrow \infty \\ &amp; \theta^{k} \rightarrow 0, \quad \text { as } k \rightarrow \infty \end{aligned}\] <p>\(\textbf{explanation:}\) This theorem means that under mild assumption, and not noised data, LISTA network verify that thresholds \(\theta^k \rightarrow 0\) and we have the weight coupling \(\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow W_{2}^{k}\) which is verified for ISTA. Next section introduce a variant of LISTA that leverages on this coupling.</p> <p>Our results for weight coupling is not convincing because of the difference in the training method (cf <a href="https://arxiv.org/abs/1808.10038">the paper</a>).</p> <div style="text-align: center"> <img src="/assets/img/lista_tmh1.PNG" alt="A sample image" style="width: 800px"/> </div> <h2><u>LISTA-CP and sufficient condition for convergence</u></h2> <p>Theorem 1 shows that we have a assymptotical weight coupling : \(\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow W_{2}^{k}\). The idea of LISTA Partial weight Coupling (LISTA-CP) is to make this asymptotical equality true for all layers. Then we replace \(W_1^k\) by \(W^k\) and \(W_2^k\) by \(\left(\mathbb{I}-W^{k} A\right)\) which gives the NN:</p> \[\begin{align} d_\theta^K(y) = S_{\theta^K}\left(\left(\mathbb{I}-W^{K} A\right) \dots S_{\theta^1}\left(\left(\mathbb{I}-W^{1} A\right)x^0 + W^{1}y\right) \dots + W^{K}y\right) \quad \quad \text{with} \quad \theta = \{W^{1}, \ldots, W^{K}, \theta^1, \ldots, \theta^K\} \end{align}\] <p>\(\textbf{Theorem 2:}\) Given \(\left\{W^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(\left\{x^{k}\right\}_{k=1}^{\infty}\) be generated by (4). If Assumption 1 holds and \(s\) is sufficiently small, then there exists a sequence of parameters \(\theta=\left\{W^{k}, \theta^{k}\right\}\) such that, for all \(\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma)\), we have the error bound:</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp (-c k)+C \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(c&gt;0, C&gt;0\) are constants that depend only on \(A\) and \(s\).</p> <p>\(\textbf{explanation:}\) This theorem means that there exists, under mild assumptions, parameters \(\theta\) such that our LISTA-CP network error estimation is bounded by a proportion of the noise level. That means that in the noiseless case, our network converge linearly to the signals in our training set. This is a better rate of convergence than ISTA which converges sublinearly.</p> <p>\(\textbf{Discussion:}\) The bound (5) clarifies the accelerated convergence of LISTA compared to ISTA. ISTA achieves a linear rate with a sufficiently large \(\lambda\):</p> \[\begin{aligned} x^{k} \rightarrow \bar{x}(\lambda) \text{ sublinearly, } &amp; \left\|\bar{x}(\lambda)-x^{*}\right\|=O(\lambda), &amp; \lambda&gt;0 \\ x^{k} \rightarrow \bar{x}(\lambda) \text{ linearly, } &amp; \left\|\bar{x}(\lambda)-x^{*}\right\|=O(\lambda), &amp; \lambda \text{ large enough. } \end{aligned}\] <p>The choice of \(\lambda\) in LASSO introduces an inherent trade-off between convergence rate and approximation accuracy in solving the inverse problem. A larger \(\lambda\) leads to faster convergence but a less accurate solution, and vice versa.</p> <p>However, if \(\lambda\) varies adaptively across iterations, a promising trade-off emerges. LISTA and LISTA-CP adopt this approach by training free thresholds \(\{\theta^{k}\}_{k=1}^{K}\). LISTA and LISTA-CP learning-based algorithms achieve accurate solutions at a fast convergence rate. Theoretical results in Theorem 2 establish the existence of such a sequence \(\{W^{k}, \theta^{k}\}_{k}\) in LISTA-CP. The following experimental results demonstrate empiricaly this convergence improvement. The recovery performance is evaluated by NMSE (in \(\mathrm{dB}\) ):</p> \[\operatorname{NMSE}\left(\hat{x}, x^{*}\right)=10 \log _{10}\left(\frac{\mathbb{E}\left\|\hat{x}-x^{*}\right\|^{2}}{\mathbb{E}\left\|x^{*}\right\|^{2}}\right)\] <div style="text-align: center"> <img src="/assets/img/lista_discussion.PNG" alt="A sample image" style="width: 550px"/> </div> <h2><u>LISTA-SS and LISTA-CPSS</u></h2> <p>LISTA-SS is a special thresholding scheme, with Support Selection (SS), which is inspired by the article <a href="https://arxiv.org/abs/1104.0262">Fast Linearized Bregman Iteration for Compressive Sensing and Sparse Denoising</a>. This technique shows advantages on recoverability and convergence.</p> <p>The layers of LISTA-SS is defined as LISTA but with a diferent actionvation function:</p> \[x^{k+1}=\eta_{\mathrm{ss}_{\theta^{k}}}^{p^{k}}\left(W_{2}^{k} x^{k} + W_{1}^{k} y\right), \quad k=0, \cdots, K-1\] <p>where \(\eta_{s s}\) is the thresholding operator with support selection, formally defined as:</p> <div style="text-align: center"> <img src="/assets/img/lista_nss.PNG" alt="A sample image" style="width: 450px"/> </div> <p>where \(S^{p^{k}}(v)\) includes the elements with the largest \(p^{k} \%\) magnitudes in vector \(v\) :</p> \[S^{p^{k}}(v)=\left\{i_{1}, i_{2}, \cdots, i_{p^{k}}|| v_{i_{1}}|\geq| v_{i_{2}}|\geq \cdots| v_{i_{p^{k}}}|\cdots \geq| v_{i_{n}} \mid\right\}\] <p>To summarize, \(p^{k}\) is a hyperparameter to be manually tuned, and \(\theta^{k}\) is a parameter to train. We use an empirical formula to select \(p^{k}\) for layer \(k: p^{k}=\min \left( k, 5\right)\).</p> <p>If we adopt the partial weight coupling we obtain LISTA-CPSS:</p> \[\begin{align} x^{k+1}=\eta_{\mathrm{ss}} \theta_{\theta^{k}}^{k}\left(\left(\mathbb{I}-W^{k} A\right)x^k + W^{k}y\right), \quad k=0,1, \cdots, K-1 \end{align}\] <p>This support selection permits to proove a second convergence theorem.</p> <p>\(\textbf{Assumption 2:}\) Signal \(x^{*}\) and observation noise \(\varepsilon\) are sampled from the following set:</p> \[\left(x^{*}, \varepsilon\right) \in \overline{\mathcal{X}}(B, \underline{B}, s, \sigma) \triangleq\left\{\left(x^{*}, \varepsilon\right)|| x_{i}^{*} \mid \leq B, \forall i,\left\|x^{*}\right\|_{1} \geq \underline{B},\left\|x^{*}\right\|_{0} \leq s,\|\varepsilon\|_{1} \leq \sigma\right\} .\] <p>\(\textbf{Theorem 3 (LISTA-CPSS):}\) Given \(\left\{W^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(\left\{x^{k}\right\}_{k=1}^{\infty}\) be generated by (6). With the same assumption and parameters as in Theorem 2, the approximation error can be bounded for all \(\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma)\):</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp \left(-\sum_{t=0}^{k-1} c_{\mathrm{ss}}^{t}\right)+C_{\mathrm{ss}} \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(c_{\mathrm{ss}}^{k} \geq c\) for all \(k\) and \(C_{\mathrm{ss}} \leq C\).</p> <p>If Assumption 2 holds, \(s\) is small enough, and \(\underline{B} \geq 2 C \sigma\) (SNR is not too small), then there exists another sequence of parameters \(\left\{\tilde{W}^{k}, \tilde{\theta}^{k}\right\}\) that yields the following improved error bound: for all \(\left(x^{*}, \varepsilon\right) \in \overline{\mathcal{X}}(B, \underline{B}, s, \sigma)\),</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp \left(-\sum_{t=0}^{k-1} \tilde{c}_{\mathrm{ss}}^{t}\right)+\tilde{C}_{\mathrm{ss}} \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(\tilde{c}_{\mathrm{ss}}^{k} \geq c\) for all \(k\), \(\tilde{c}_{\mathrm{ss}}^{k}&gt;c\) for large enough \(k\), and \(\tilde{C}_{\mathrm{ss}}&lt;C\).</p> <p>The bound in (7) ensures that, with the same assumptions and parameters, LISTA-CPSS is at least no worse than LISTA-CP. The bound in (8) shows that, under stronger assumptions, LISTA-CPSS can be strictly better than LISTA-CP in both folds: \(\tilde{c}_{\mathrm{ss}}^{k}&gt;c\) is the better convergence rate of LISTA-CPSS; \(\tilde{C}_{\mathrm{ss}}&lt;C\) means that the LISTA-CPSS can achieve smaller approximation error than the minimum error that LISTA can achieve. The following results suggests that we can learn parameters of the theorem 2 and 3.LISTA_CP ,LISTA_SS and LISTA_CPSS reach better results than LISTA.</p> <div style="text-align: center"> <img src="/assets/img/lista_thm3.PNG" alt="A sample image" style="width: 550px"/> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[This project aims to implement the first unfolded neural network described in "Learning fast approximations of sparse coding" and 3 variants of it introduced in the article "Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds" which gives theoritical guarantees of convergence.]]></summary></entry><entry><title type="html">Colour transfert with optimal transport method</title><link href="https://chappone.github.io/blog/2023/color_transport/" rel="alternate" type="text/html" title="Colour transfert with optimal transport method"/><published>2023-11-05T15:12:00+00:00</published><updated>2023-11-05T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2023/color_transport</id><content type="html" xml:base="https://chappone.github.io/blog/2023/color_transport/"><![CDATA[<h2><u>Introduction</u></h2> <p>The objective is to transfer the colormap of an image to make it match that of a target image using optimal transport theory. The code to reproduce the results is available on my <a href="https://github.com/ChapponE/optimal_transport_color_transfer">GitHub account</a>. The algorithm developed and theory come from the articles <a href="https://mural.maynoothuniversity.ie/15125/">Automated colour grading using colour distribution transfer</a> and <a href="https://www.semanticscholar.org/paper/Wasserstein-Barycenter-and-Its-Application-to-Rabin-Peyr%C3%A9/9b208891d1287ebb5b84ac801b41c3313d7e3303">Wasserstein Barycenter and Its Application to Texture Mixing</a>.</p> <h2><u>Representation of the images</u></h2> <p>Because we are interested only in the colors, one image is modeled as a point cloud of \(\mathbb{R}^3\) with a number of points equal to the number of pixels in the image. This means that \(X=\{x_i\}_{i \in [1,N]}\) with \(x_i \in \mathbb{R}^3\) represents an image. If we permute the pixels of an image, it’s representation remains the same. Thus we work in: <br/> <span style="display: block; text-align: center;">\(\mathcal{M}=\{[X]; X \in (\mathbb{R}^3)^N \}\) with \([X]=\{(X_{\sigma(j)})_{j=[1,N]}, \sigma \in \Sigma_N\}\), \(\Sigma_N\) is the set of all permutations.</span></p> <h2><u>Wasserstein Distance</u></h2> <p>We define the Wasserstein distance on \(\mathcal{M}\) :<br/> <span style="display: block; text-align: center;">\(\forall X,Y \in \mathcal{M}, W(X, Y)^{2}={\min\limits_{\sigma \in \Sigma_{N}}} W_{\sigma}(X, Y) \quad \text { where } \quad W_{\sigma}(X, Y)={\sum\limits_{i \in I}}\left\|X_{i}-Y_{\sigma(i)}\right\|^{2}\) </span> This minimization problem can be computationally expensive, but we can leverage the scalar case (when \(X_i\) and \(Y_i\) are in \(\mathbb{R}\) for \(i \in [1, N]\) instead of being in \((\mathbb{R}^3)^N\)).<br/> In this case \(W(X, Y)=W_{\sigma^*}(X, Y)\) with \(\sigma^*=\sigma_Y \circ \sigma_X^{-1}\) where :<br/> <span style="display: block; text-align: center;">\(\forall 1 \leq i \leq N, \quad X_{\sigma_{X}(i)} \leq X_{\sigma_{X}(i+1)} \quad \text { and } \quad Y_{\sigma_{Y}(i)} \leqslant Y_{\sigma_{Y}(i+1)}\)</span> It is not computationally costly to calculate \(\sigma\) with fast sorting algorithms. <br/> Then we define the sliced Wasserstein Distance \(\tilde{W}\) which is calculated with only 1-D Wasserstein Distances and approximates the wesserstein Distance as follows:<br/> <span style="display: block; text-align: center;">\(\forall X, Y \in \mathcal{M}, \tilde{W}(X, Y)^{2}=\int_{\theta \in S^2} {\min\limits_{\sigma_{\theta} \in \Sigma_{N}}} {\sum\limits_{i \in I}} \left\langle X_{i}-Y_{\sigma_{\theta}(i)}, \theta\right\rangle^{2} \mathrm{~d} \theta\)</span> This calculate the Wasserstein Distance using the projected vectors in a certain direction of \(\mathbb{R}^3\) and integrates over all directions. For more details, you can refer to the articles mentioned in the introduction.</p> <h2><u>Barycenter in Wasserstain Space</u></h2> <p>Here we define the barycenter in this space and provide an algorithm to obtain the barycenter of the image we want to transport, weighted at 0, and the image with the desired colormap, weighted at 1. <br/> By analogy with the barycenter in Euclidean space, the barycenter in \((\mathcal{M}, \tilde{W})\) is defined as :<br/> \(\operatorname{Bar}\left(\lambda_{j}, Y^{j}\right)_{j \in J} \in \underset{X}{\operatorname{argmin}} E(X)\) where \(E(X)={\sum\limits_{j \in J}}\lambda_{j} \tilde{W}\left(X_\theta, Y_\theta^{j}\right)^{2}=\int_{\theta \in S^2}{\sum\limits_{j \in J}}\lambda_j \left\langle X_{i}-Y_{\sigma^*_{\theta}(i)}, \theta\right\rangle^{2} \mathrm{~d} \theta\)<br/> With \(X_\theta=\{&lt;X_i,\theta&gt;\}_{i\in[1,N]}\subset \mathbb{R}\) and the \(\sigma^*_\theta\) defined in Wasserstein Distance part.</p> <h2><u>The transportation algorithm:</u></h2> <p>We apply the Wasserstein barycenter with \(\{(0,X);(1,Y)\}\) with \(X\) as the original image (Image 1 in the following figure) we want to transform and \(Y\) as the target image (one of the 5 others) that we want to use to transform \(X\) and use colors.</p> <div style="text-align: center"><img src="/assets/img/ot_6img.PNG" alt="A sample image" width="600"/></div> <p>\(\,\) <br/> Thus the transported image is defined as:<br/> \(\begin{align} X^*=\arg{\min\limits_X}\int_{\theta \in S^2}W(X_\theta, Y_\theta)^2 \mathrm{~d} \theta\end{align}\)<br/> We can use gradient descent to find an approximation of this minimizer:<br/> <span><span style="font-weight: bold;">Gradient descent to minimize (1):</span><br/> <span style="font-weight: bold;">Assumption:</span> Images \(X\) and \(Y\) have the same number of pixels.<br/> <span style="font-weight: bold;">Input:</span> \(X\), \(Y\) and a stepsize \(\epsilon\)<br/> \(X^{0}=X\)<br/> <span style="font-weight: bold;">For</span> \(k = 0, 1, 2, \ldots\):<br/> \(\quad \quad X^{k}=X^{k}-\epsilon\nabla W(X^{k}_\theta, Y_\theta)\) with \(\theta\) a realization of an uniform random variable in \(S^2\)<br/> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{k\rightarrow\infty}{X^k} \in \arg{\min\limits_X}\int_{\theta \in S^2}W(X_\theta, Y_\theta)^2 \mathrm{~d} \theta\) </span></p> <ul> <li>Remark: At each step, we pick a direction in the color space and take a step of gradient descent to make the colormap of the current iteration closer to the one of \(Y\) in that direction.</li> </ul> <h2><u>Results</u></h2> <p>Here are the results obtained by applying the algorithm with 100 iterations and \(\epsilon=1\) to transport the first image, \(X\), to the colormap of the 5 other images, denoted as \(Y\):</p> <p><img src="/assets/img/Transported_images.png" alt="A sample image" style="width: 100%; height: auto; margin: 0; position: absolute; left: 0;"/></p> <h2>$$\;$$</h2> <h2>$$\;$$</h2> <h2>$$\;$$</h2> <h2>$$\;$$</h2> <h2>$$\;$$</h2> <h2>$$\;$$</h2> <h2>$$\;$$</h2>]]></content><author><name></name></author><summary type="html"><![CDATA[This project aims to match the color palette of an original image with that of a target image. It was developed as part of a practical assignment in an optimal transport class.]]></summary></entry><entry><title type="html">Unfolded neural network for deblur and denoise</title><link href="https://chappone.github.io/blog/2023/mat/" rel="alternate" type="text/html" title="Unfolded neural network for deblur and denoise"/><published>2023-10-16T15:12:00+00:00</published><updated>2023-10-16T15:12:00+00:00</updated><id>https://chappone.github.io/blog/2023/mat</id><content type="html" xml:base="https://chappone.github.io/blog/2023/mat/"><![CDATA[<h2><u>Introduction</u></h2> <p>This project aims to deblur and denoise images, with a focus on the MNIST dataset. The methodology is based on the “Unfolded Forward Backward” approach, developed as part of <a href="https://chappone.github.io/ws/assets/pdf/circumstellar.pdf">my master’s thesis project</a> at ENS Lyon. The code to produce the results of this blog post is available on my <a href="https://github.com/ChapponE/img_deblur">github account</a>. I introduced in an <a href="https://chappone.github.io/ws/blog/2023/lista/">other post blog</a> unfolded neural network with the first model introduced in the article <a href="https://icml.cc/Conferences/2010/papers/449.pdf">Learning Fast Approximations of Sparse Nonlinear Regression</a></p> <h2><u>Data</u></h2> <p>I worked with the standard MNIST dataset imported from the Keras library, consisting of 70,000 images, each of size \(28 \times 28\) pixels. To normalize these images, I divided each pixel’s value by 255 to bring them into the range of 0 to 1. To degrade the quality of the handwritten digits, I applied a Gaussian blur with a kernel size of \(7 \times 7\) and a standard deviation of 0.5. Subsequently, I introduced Gaussian noise with a standard deviation of 0.25, resulting in pairs of original and degraded images, as displayed below. We measured the difference between the degraded and original images using the traditional PSNR metric.</p> <div style="text-align: center"><img src="/assets/img/sample.png" alt="A sample image" width="320"/></div> <p>The main objective of this project is to create an estimator for the original image based on the degraded image. The quality of this estimator is assessed using the PSNR metric.</p> <h2><u>Model</u></h2> <p>\(\bullet\) I adopted the classical formalism of inverse problems:<br/> The goal of solving an \(\textit{inverse problem}\) is to create an estimator, denoted as \(\boldsymbol{\widehat{x}} \in \mathbb{R}^{28^2}\), from a set of observed measurements, represented as the degraded images \(\boldsymbol{y} \in \mathbb{R}^{28^2}\). The objective is to create an estimator that is close to the ground truth data, the original image \(\boldsymbol{\bar{x}}\). In our case, we have \(\boldsymbol{y} = A\boldsymbol{\bar{x}} + \mathbf{\epsilon}, \text{ with } A \in \mathbb{R}^{28^2 \times 28^2} \text{ and } \mathbf{\epsilon} \sim \mathbb{N}(\mathbf{0}, \mathbf{\sigma^2 I})\). With \(A\) the gaussian blur and \(\mathbf{\epsilon}\) the gaussian noise.</p> <p>\(\bullet\) The estimator \(\boldsymbol{\widehat{x}}\) for \(\boldsymbol{\bar{x}}\) is typically obtained by solving a variational problem, defined as follows: \(\begin{align} \widehat{\boldsymbol{x}} \in \underset{\boldsymbol{x} \in \mathbb{R}^{28^2}}{\arg \min}\left[\frac{1}{2}||\boldsymbol{y}-A\boldsymbol{x}||_2^2+\mathcal{P}(\boldsymbol{x})\right] \end{align}\) Here, \(||\boldsymbol{y}-A\boldsymbol{x}||_2^2\) represents the data fidelity term, ensuring that the degraded solution \(A\boldsymbol{\widehat{x}}\) is close to the degraded signal \(\boldsymbol{y}\). Additionally, \(\mathcal{P}: \mathbb{R}^{28^2} \rightarrow \mathbb{R}\) is a prior term. For more details on variational problems, you can refer to my Master’s thesis.</p> <p>\(\bullet\) Choosing an appropriate prior term can be challenging. However, once a suitable prior term is chosen, it is possible to find the minimizer of equation (1) using proximal iterative methods like Forward-Backward:<br/> <span><span style="font-weight: bold;">Forward-Backward algorithm:</span><br/> <span style="font-weight: bold;">Assumption:</span> \(\zeta := |||A^*A|||_2&lt;2,\)<br/> <span style="font-weight: bold;">Input:</span> \(\boldsymbol{y}\in \mathbb{R}^{28^2}\), step size \(\tau\in ]0,2\zeta^{-1}[,\)<br/> \(x^0 = A^*y,\)<br/> <span style="font-weight: bold;">For</span> \(n = 0, 1, 2, \ldots\):<br/> \(\quad \quad \tilde{x}^{n}=x^n-\tau A^*(Ax^n - \boldsymbol{y})\);<br/> \(\quad \quad x^{n+1} = \operatorname{prox}_{\tau \mathcal{R}}(\tilde{x}^{n})\);<br/> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{n\rightarrow\infty}{x^n} \in{\arg\min\limits_{x \in \mathbb{R}^{28^2}}}\left[\dfrac{1}{2}||\boldsymbol{y}-A\boldsymbol{x}||_2^2+\mathcal{P}(\boldsymbol{x})\right]\) </span></p> <ul> <li>Remark: Let \(h: \mathcal{H} \rightarrow \mathbb{R},\) \(\forall x \in \mathcal{H} \quad \operatorname{prox}_{\tau h}(x)=\underset{y \in \mathcal{H}}{\operatorname{argmin}} \frac{1}{2 \tau}\|x-y\|^{2}+h(y)\)</li> </ul> <p>\(\bullet\) To address the challenge of selecting a suitable prior term, the iterations of Forward-Backward can be unfolded to learn the prior through deep-learning frameworks. The first unfolded network was introduced in the paper <a href="https://icml.cc/Conferences/2010/papers/449.pdf"> Learning Fast Approximations of Sparse Coding </a> in 2010. To parameterize the unfolded network, I replaced the proximal operator of the prior with UNets, denoted as \(U_{\theta^k}\). To facilitate the learning process, I limited the number of iterations to 5. The algorithm is defined as follows:\(\\\)</p> <p><span><span style="font-weight: bold;">Unfolded Forward-Backward architecture network:</span><br/> <span style="font-weight: bold;">Assumption:</span> \(\zeta:=|||A^*A|||_2 &lt; 2,\, {U_{\theta^k}}_{k\in[0;4]},\) UNets,<br/> <span style="font-weight: bold;">Input:</span> \(\boldsymbol{y}\in \mathbb{R}^{28^2}\), step size \(\tau \in ]0,2\zeta^{-1}[,\)<br/> \(x^0 = A^*y,\)<br/> <span style="font-weight: bold;">For</span> \(n = 0, 1, 2, \ldots 4\):<br/> \(\quad \quad \tilde{x}^{n}=x^n- \tau A^*(y - Ax^n);\)<br/> \(\quad \quad x^{n+1} = U_{\theta^{n+1}}(\tilde{x}^{n})\);<br/> <span style="font-weight: bold;">Output:</span> \(f_\theta(y)=x^5\) </span></p> <p>\(\bullet\) I trained the unfolded network using \(300\) of the \(70\,000\) images from the MNIST dataset. I reserved \(10\,000\) images for testing. It is possible to play with the number of iterations, which corresponds to the layers of our neural network, and the number of scales of the UNets, which determine the size of the UNets. I kept the number of layers fixed at 5 and the number of scales within the set \(\{2, 3, 4\}\).</p> <h2><u>Results</u></h2> <p>\(\bullet\) I minimized the cost function, which is defined as the mean squared error on the \(300\) training images: \(\sum\limits_{i \in [1,300]} \|\bar{x}_i - f_\theta(y)\|^2_2\).</p> <p>\(\bullet\) In the following plot, the training was performed for 50 epochs using the Adam algorithm. The red curve represents the PSNR calculated on the test set, and the blue curve represents the value of the loss function with respect to the number of epochs. Additionally, a table displaying the mean PSNR on the test set, the number of parameters, and the training time is provided.</p> <div style="text-align: center"><img src="/assets/img/curves_mnist.PNG" alt="A sample image" width="820"/></div> <div style="text-align: center"><img src="/assets/img/table_MNIST.png" alt="A sample image" width="490"/></div> <p>\(\bullet\) It is evident that there is no significant difference between the three settings. This can be attributed to the relatively low number of training epochs. In my thesis, it was observed that a higher number of parameters leads to better PSNR results. However, training for hundreds of epochs on my personal computer, which lacks a GPU, is time-consuming. Additionally, it’s noteworthy that the training time increases with a higher number of scales. <br/> Here is an example of a reconstructed sample, demonstrating the quality of the results:</p> <div style="text-align: center"><img src="/assets/img/sample_mnist.png" alt="A sample image" width="820"/></div>]]></content><author><name></name></author><summary type="html"><![CDATA[This project aims to deblur and denoise images, with a focus on the MNIST dataset. The methodology is based on the "Unfolded Forward Backward" approach, developed as part of my master's thesis project at ENS Lyon.]]></summary></entry></feed>