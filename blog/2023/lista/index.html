<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Edouard Chappon</title> <meta name="author" content="Edouard Chappon"> <meta name="description" content="This project aims to implement the first unfolded neural network described in " learning fast approximations of sparse coding and variants it introduced in the article linear convergence unfolded ista its practical weights thresholds which gives theoritical guarantees convergence.> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favi.png?b71398e59abbbceb51d3383fe55afffb"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chappone.github.io/blog/2023/lista/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="theme-red"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Edouard </span>Chappon</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Origins of unfolded networks and theory; Learning Iterative Soft Thresholding Algorithm (LISTA)</h1> <p class="post-meta">December 3, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2><u>Introduction</u></h2> <p>The objective is to implement the first unfolded Neural Network (NN) described in <a href="https://icml.cc/Conferences/2010/papers/449.pdf" rel="external nofollow noopener" target="_blank">Learning Fast Approximations of Sparse Nonlinear Regression</a>. I also implement 3 variants of it introduced in <a href="https://arxiv.org/abs/1808.10038" rel="external nofollow noopener" target="_blank"> Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds </a> which gives the proof of 3 theorems we will explain and empiricaly verify. <br> The code used for this project is available on <a href="https://github.com/ChapponE/LISTA" rel="external nofollow noopener" target="_blank">GitHub</a>. I used other unroled network to learn how to deblur and denoise images from mnist daset in an <a href="https://chappone.github.io/ws/blog/2023/mat/">other post blog</a><a>.</a></p> <h2><u>Sparse signal recovery and Data</u></h2> <p>In this blog post we will solve an sparse signal recovery problem. We aim to recover a sparse vector \(x^* \in \mathbb{R}^n\) from its noisy linear measurments \(y \in \mathbb{R}^m\), with \(m=250\) and \(n=500\): <span style="display: block; text-align: center;"> \(y=Ax^*+\epsilon\)</span> Where \(\epsilon \in \mathbb{R}^m\) an additive Gaussian white noise to have a Signal to Noise Rate (SNR) of \(30dB\), and \(A \in \mathbb{R}^{m\times n}\) with each entries sampled as Gaussian distribution \(A_{i,j}∼\mathcal{N}(0,1/m)\) and then we normalize the columns.<br> We define \(x^*\) as a Gaussian vector of \(\mathbb{R}^n\) where we set each entry to \(0\) with a probability of \(p_b=0.1\) to make it sparse. \(1\,000\) such data are generated for the train set and \(1\,000\) others for the test set.</p> <h2><u>Iterative Shrinkage Thresholding Algorithm (ISTA)</u></h2> <p>In inverse problem, the estimator \(\widehat{x}\) of \(x^*\) is classically obtained by the minimization of the sum of a datafidelity term \(||y-Ax||_2^2\) and the regularization \(\lambda \|x\|_1\) to enforce the sparsity of the solution. This problem is known as LASSO problem:<br> \(\begin{align} \widehat{x} \in \underset{x \in \mathbb{R}^{n}}{\arg\min}\left[\dfrac{1}{2}||y-Ax||_2^2+ \lambda \|x\|_1 \right] \end{align}\)</p> <p>ISTA is an iterative algorithm which aims to solve \((1)\), it converges sublinearly (see <a href="https://nikopj.github.io/blog/understanding-ista/" rel="external nofollow noopener" target="_blank">the link</a> for more explanations of that algorithm):</p> <p><span><span style="font-weight: bold;">ISTA:</span><br> <span style="font-weight: bold;">Assumption:</span> \(\zeta := |||A^*A|||_2&lt;2,\)<br> <span style="font-weight: bold;">Input:</span> \(y\in \mathbb{R}^{m}\), step size \(\tau\in ]0,2\zeta^{-1}[,\)<br> \(x^0 = A^*y,\)<br> <span style="font-weight: bold;">For</span> \(k = 0, 1, 2, \ldots\):<br> \(\quad \quad \tilde{x}^{k}=x^n-\tau A^*(Ax^k - y)\)<br> \(\quad \quad x^{k+1} = \mathcal{S}_{\lambda/L}(\tilde{x}^{k}) \quad \quad \text{with} \; \;(\mathcal{S}_{\theta}(x))_i = \operatorname{sgn}(x_i)\operatorname{max}(|x_i|-\theta,0)\)<br> <span style="font-weight: bold;">Output:</span> \(\lim\limits_{k\rightarrow\infty}{x^k} \in{\arg\min\limits_{x \in \mathbb{R}^{n}}}\left[\dfrac{1}{2}||y-Ax||_2^2+\lambda||x||_1\right]\) </span></p> <ul> <li>Remark: \(\mathcal{S}_\theta\) is called the soft thresholding function. It is a non-linear function.</li> </ul> <h2><u>Original LISTA</u></h2> <p>A neural network \(d_\theta\) with \(K\) layers and parametrized by \(\theta\) is defined as:</p> \[\begin{align} d_\theta(y) = \eta^{K}\left(W^{K}_2 \dots \eta^{1}\left(W^{1}_2y + W^{1}_1\right) \dots + W^{K}_1\right) \quad\quad \text{with} \quad \theta = \{W^{1}_2, \ldots, W^{K}_2, W^{1}_1, \ldots, W^{K}_1\} \end{align}\] <p>The article <a href="https://arxiv.org/abs/2010.13490" rel="external nofollow noopener" target="_blank">Learning Fast Approximations of Sparse Nonlinear Regression</a> introduce the first unrolled neural network called Learned ISTA (LISTA) by hilighting the fact that an iteration of ISTA looks like a layer of a neural network if we rearange iterations of ISTA as below and we assimilate \(\mathcal{S}_{\theta^k}\) as the activation function:</p> <div style="text-align: center"> <img src="/assets/img/ista_vs_nn.PNG" alt="A sample image" style="width: 550px"> </div> <p>We fix number of layers \(K=16\) and \(x^0=\vec{0}\) LISTA network is defined as :</p> \[\begin{align} d_\theta^K(y) = S_{\theta^K}\left(W^{K}_2 \dots S_{\theta^1}\left(W^{1}_2x^0 + W^{1}_1y\right) \dots + W^{K}_1y\right) \quad \quad \text{with} \quad \theta = \{W^{1}_2, \ldots, W^{K}_2, \theta^1, \ldots, \theta^K, W^{1}_1, \ldots, W^{K}_1\} \end{align}\] <ul> <li>Remarks: Advantage of LISTA compare to ISTA is that when the parameters \(\theta\) is trained it is cheap \(\quad \quad\quad \;\) to evaluate. ISTA requires thousands of iterations where LISTA requires only few tens. \(\quad \quad\quad \;\) This network and ones we will see further have been trained with Adam for \(2\,500\) epochs. \(\quad \quad\quad \;\) With thoses settings, LISTA has \(6\,000\,016\) parameters to train.</li> </ul> <p>To start with a good initialization and make training easier, we start training with :</p> <div style="text-align: center"> $$\begin{align*} \forall k \in [1,\ldots, K], \quad&amp;\theta^k = \lambda/L \\ &amp; W_2^k = (\mathbb{I}-\tau A^*A) \quad \quad \text{with} \quad \tau=L^{-1}\\ &amp; W_1^k = -\tau A^* \end{align*}$$ </div> <p>Here are the curves of loss and PSNR of the traini and validation/test sets (which contains \(1\,000\) data each) during the training phase. The next parts explains and illustrates theoriticals results of convergence for LISTA networks and 3 variations.</p> <div style="text-align: center"> <img src="/assets/img/lista_train_curves.PNG" alt="A sample image" style="width: 550px"> </div> <h2><u>Necessary condition for LISTA convergence</u></h2> <p>Theorems are prooved in the paper <a href="https://arxiv.org/abs/1808.10038" rel="external nofollow noopener" target="_blank"> Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds</a>.</p> <p>\(\textbf{Assumption 1:}\) The signal \(x^{*}\) and the observation noise \(\epsilon\) are sampled from the following set:</p> \[\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma) \triangleq\left\{\left(x^{*}, \varepsilon\right)|| x_{i}^{*} \mid \leq B, \forall i,\left\|x^{*}\right\|_{0} \leq s,\|\varepsilon\|_{1} \leq \sigma\right\} .\] <p>\(\textbf{Theorem 1:}\) Given \(\theta=\left\{W_{1}^{k}, W_{2}^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(y\) be observed by (1) and \(\left\{d^k_\theta\right\}_{k=1}^{\infty}\) be generated layer-wise by LISTA (3). If the following holds uniformly for any \(\left(x^{*}, \epsilon\right) \in \mathcal{X}(B, s, 0)\):</p> \[d^k_\theta(y) \rightarrow x^{*}, \quad \text { as } k \rightarrow \infty\] <p>and \(\left\{W_{2}^{k}\right\}_{k=1}^{\infty}\) are bounded:</p> \[\left\|W_{2}^{k}\right\|_{2} \leq B_{W}, \quad \forall k=0,1,2, \cdots\] <p>then \(\theta=\left\{W_{1}^{k}, W_{2}^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) must satisfy:</p> \[\begin{aligned} &amp; W_{2}^{k}-\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow 0, \quad \text { as } k \rightarrow \infty \\ &amp; \theta^{k} \rightarrow 0, \quad \text { as } k \rightarrow \infty \end{aligned}\] <p>\(\textbf{explanation:}\) This theorem means that under mild assumption, and not noised data, LISTA network verify that thresholds \(\theta^k \rightarrow 0\) and we have the weight coupling \(\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow W_{2}^{k}\) which is verified for ISTA. Next section introduce a variant of LISTA that leverages on this coupling.</p> <p>Our results for weight coupling is not convincing because of the difference in the training method (cf <a href="https://arxiv.org/abs/1808.10038" rel="external nofollow noopener" target="_blank">the paper</a>).</p> <div style="text-align: center"> <img src="/assets/img/lista_tmh1.PNG" alt="A sample image" style="width: 800px"> </div> <h2><u>LISTA-CP and sufficient condition for convergence</u></h2> <p>Theorem 1 shows that we have a assymptotical weight coupling : \(\left(\mathbb{I}-W_{1}^{k} A\right) \rightarrow W_{2}^{k}\). The idea of LISTA Partial weight Coupling (LISTA-CP) is to make this asymptotical equality true for all layers. Then we replace \(W_1^k\) by \(W^k\) and \(W_2^k\) by \(\left(\mathbb{I}-W^{k} A\right)\) which gives the NN:</p> \[\begin{align} d_\theta^K(y) = S_{\theta^K}\left(\left(\mathbb{I}-W^{K} A\right) \dots S_{\theta^1}\left(\left(\mathbb{I}-W^{1} A\right)x^0 + W^{1}y\right) \dots + W^{K}y\right) \quad \quad \text{with} \quad \theta = \{W^{1}, \ldots, W^{K}, \theta^1, \ldots, \theta^K\} \end{align}\] <p>\(\textbf{Theorem 2:}\) Given \(\left\{W^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(\left\{x^{k}\right\}_{k=1}^{\infty}\) be generated by (4). If Assumption 1 holds and \(s\) is sufficiently small, then there exists a sequence of parameters \(\theta=\left\{W^{k}, \theta^{k}\right\}\) such that, for all \(\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma)\), we have the error bound:</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp (-c k)+C \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(c&gt;0, C&gt;0\) are constants that depend only on \(A\) and \(s\).</p> <p>\(\textbf{explanation:}\) This theorem means that there exists, under mild assumptions, parameters \(\theta\) such that our LISTA-CP network error estimation is bounded by a proportion of the noise level. That means that in the noiseless case, our network converge linearly to the signals in our training set. This is a better rate of convergence than ISTA which converges sublinearly.</p> <p>\(\textbf{Discussion:}\) The bound (5) clarifies the accelerated convergence of LISTA compared to ISTA. ISTA achieves a linear rate with a sufficiently large \(\lambda\):</p> \[\begin{aligned} x^{k} \rightarrow \bar{x}(\lambda) \text{ sublinearly, } &amp; \left\|\bar{x}(\lambda)-x^{*}\right\|=O(\lambda), &amp; \lambda&gt;0 \\ x^{k} \rightarrow \bar{x}(\lambda) \text{ linearly, } &amp; \left\|\bar{x}(\lambda)-x^{*}\right\|=O(\lambda), &amp; \lambda \text{ large enough. } \end{aligned}\] <p>The choice of \(\lambda\) in LASSO introduces an inherent trade-off between convergence rate and approximation accuracy in solving the inverse problem. A larger \(\lambda\) leads to faster convergence but a less accurate solution, and vice versa.</p> <p>However, if \(\lambda\) varies adaptively across iterations, a promising trade-off emerges. LISTA and LISTA-CP adopt this approach by training free thresholds \(\{\theta^{k}\}_{k=1}^{K}\). LISTA and LISTA-CP learning-based algorithms achieve accurate solutions at a fast convergence rate. Theoretical results in Theorem 2 establish the existence of such a sequence \(\{W^{k}, \theta^{k}\}_{k}\) in LISTA-CP. The following experimental results demonstrate empiricaly this convergence improvement. The recovery performance is evaluated by NMSE (in \(\mathrm{dB}\) ):</p> \[\operatorname{NMSE}\left(\hat{x}, x^{*}\right)=10 \log _{10}\left(\frac{\mathbb{E}\left\|\hat{x}-x^{*}\right\|^{2}}{\mathbb{E}\left\|x^{*}\right\|^{2}}\right)\] <div style="text-align: center"> <img src="/assets/img/lista_discussion.PNG" alt="A sample image" style="width: 550px"> </div> <h2><u>LISTA-SS and LISTA-CPSS</u></h2> <p>LISTA-SS is a special thresholding scheme, with Support Selection (SS), which is inspired by the article <a href="https://arxiv.org/abs/1104.0262" rel="external nofollow noopener" target="_blank">Fast Linearized Bregman Iteration for Compressive Sensing and Sparse Denoising</a>. This technique shows advantages on recoverability and convergence.</p> <p>The layers of LISTA-SS is defined as LISTA but with a diferent actionvation function:</p> \[x^{k+1}=\eta_{\mathrm{ss}_{\theta^{k}}}^{p^{k}}\left(W_{2}^{k} x^{k} + W_{1}^{k} y\right), \quad k=0, \cdots, K-1\] <p>where \(\eta_{s s}\) is the thresholding operator with support selection, formally defined as:</p> <div style="text-align: center"> <img src="/assets/img/lista_nss.PNG" alt="A sample image" style="width: 450px"> </div> <p>where \(S^{p^{k}}(v)\) includes the elements with the largest \(p^{k} \%\) magnitudes in vector \(v\) :</p> \[S^{p^{k}}(v)=\left\{i_{1}, i_{2}, \cdots, i_{p^{k}}|| v_{i_{1}}|\geq| v_{i_{2}}|\geq \cdots| v_{i_{p^{k}}}|\cdots \geq| v_{i_{n}} \mid\right\}\] <p>To summarize, \(p^{k}\) is a hyperparameter to be manually tuned, and \(\theta^{k}\) is a parameter to train. We use an empirical formula to select \(p^{k}\) for layer \(k: p^{k}=\min \left( k, 5\right)\).</p> <p>If we adopt the partial weight coupling we obtain LISTA-CPSS:</p> \[\begin{align} x^{k+1}=\eta_{\mathrm{ss}} \theta_{\theta^{k}}^{k}\left(\left(\mathbb{I}-W^{k} A\right)x^k + W^{k}y\right), \quad k=0,1, \cdots, K-1 \end{align}\] <p>This support selection permits to proove a second convergence theorem.</p> <p>\(\textbf{Assumption 2:}\) Signal \(x^{*}\) and observation noise \(\varepsilon\) are sampled from the following set:</p> \[\left(x^{*}, \varepsilon\right) \in \overline{\mathcal{X}}(B, \underline{B}, s, \sigma) \triangleq\left\{\left(x^{*}, \varepsilon\right)|| x_{i}^{*} \mid \leq B, \forall i,\left\|x^{*}\right\|_{1} \geq \underline{B},\left\|x^{*}\right\|_{0} \leq s,\|\varepsilon\|_{1} \leq \sigma\right\} .\] <p>\(\textbf{Theorem 3 (LISTA-CPSS):}\) Given \(\left\{W^{k}, \theta^{k}\right\}_{k=0}^{\infty}\) and \(x^{0}=0\), let \(\left\{x^{k}\right\}_{k=1}^{\infty}\) be generated by (6). With the same assumption and parameters as in Theorem 2, the approximation error can be bounded for all \(\left(x^{*}, \varepsilon\right) \in \mathcal{X}(B, s, \sigma)\):</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp \left(-\sum_{t=0}^{k-1} c_{\mathrm{ss}}^{t}\right)+C_{\mathrm{ss}} \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(c_{\mathrm{ss}}^{k} \geq c\) for all \(k\) and \(C_{\mathrm{ss}} \leq C\).</p> <p>If Assumption 2 holds, \(s\) is small enough, and \(\underline{B} \geq 2 C \sigma\) (SNR is not too small), then there exists another sequence of parameters \(\left\{\tilde{W}^{k}, \tilde{\theta}^{k}\right\}\) that yields the following improved error bound: for all \(\left(x^{*}, \varepsilon\right) \in \overline{\mathcal{X}}(B, \underline{B}, s, \sigma)\),</p> \[\begin{align} \left\|d^k_\theta(y)-x^{*}\right\|_{2} \leq s B \exp \left(-\sum_{t=0}^{k-1} \tilde{c}_{\mathrm{ss}}^{t}\right)+\tilde{C}_{\mathrm{ss}} \sigma, \quad \forall k=1,2, \cdots \end{align}\] <p>where \(\tilde{c}_{\mathrm{ss}}^{k} \geq c\) for all \(k\), \(\tilde{c}_{\mathrm{ss}}^{k}&gt;c\) for large enough \(k\), and \(\tilde{C}_{\mathrm{ss}}&lt;C\).</p> <p>The bound in (7) ensures that, with the same assumptions and parameters, LISTA-CPSS is at least no worse than LISTA-CP. The bound in (8) shows that, under stronger assumptions, LISTA-CPSS can be strictly better than LISTA-CP in both folds: \(\tilde{c}_{\mathrm{ss}}^{k}&gt;c\) is the better convergence rate of LISTA-CPSS; \(\tilde{C}_{\mathrm{ss}}&lt;C\) means that the LISTA-CPSS can achieve smaller approximation error than the minimum error that LISTA can achieve. The following results suggests that we can learn parameters of the theorem 2 and 3.LISTA_CP ,LISTA_SS and LISTA_CPSS reach better results than LISTA.</p> <div style="text-align: center"> <img src="/assets/img/lista_thm3.PNG" alt="A sample image" style="width: 550px"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>